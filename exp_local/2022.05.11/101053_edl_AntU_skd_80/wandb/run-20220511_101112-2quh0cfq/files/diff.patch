diff --git a/agent/ddpg.py b/agent/ddpg.py
index 2b8993a..f6ffaea 100644
--- a/agent/ddpg.py
+++ b/agent/ddpg.py
@@ -252,8 +252,8 @@ class DDPGAgent:
 
         # optimize critic
         if self.encoder_opt is not None:
-            self.encoder_opt.zero_grad(set_to_none=True)
-        self.critic_opt.zero_grad(set_to_none=True)
+            self.encoder_opt.zero_grad() # set_to_none=True)
+        self.critic_opt.zero_grad() # set_to_none=True)
         critic_loss.backward()
         self.critic_opt.step()
         if self.encoder_opt is not None:
@@ -273,7 +273,7 @@ class DDPGAgent:
         actor_loss = -Q.mean()
 
         # optimize actor
-        self.actor_opt.zero_grad(set_to_none=True)
+        self.actor_opt.zero_grad() # set_to_none=True)
         actor_loss.backward()
         self.actor_opt.step()
 
diff --git a/agent/sac.py b/agent/sac.py
index 8820240..f6e1422 100755
--- a/agent/sac.py
+++ b/agent/sac.py
@@ -158,6 +158,7 @@ class SACAgent:
                  action_range,
                  device,
                  encoder_lr, 
+                 diayn_lr,
                  actor_lr,
                  critic_lr,
                  alpha_lr,
@@ -175,7 +176,8 @@ class SACAgent:
                  use_tb,
                  use_wandb,
                  log_std_bounds,
-                 meta_dim=0):
+                 meta_dim=0,
+                 **kwargs):
         self.dtype = dtype
         self.maze_type = maze_type
         self.reward_free = reward_free
@@ -185,6 +187,7 @@ class SACAgent:
         self.action_range = action_range
         self.hidden_dim = hidden_dim
         self.encoder_lr = encoder_lr
+        self.diayn_lr = diayn_lr
         self.actor_lr = actor_lr
         self.critic_lr = critic_lr
         self.alpha_lr = alpha_lr
@@ -264,14 +267,19 @@ class SACAgent:
         return self.log_alpha.exp()
     
 
+    # TODO: fine-tuning 시에 쓰는 것 같아서 안건드렸음
     def init_from(self, other):
+        ipdb.set_trace()
         # copy parameters over
         utils.hard_update_params(other.encoder, self.encoder)
         utils.hard_update_params(other.diayn, self.diayn)
         utils.hard_update_params(other.actor, self.actor)
+        utils.hard_update_params(other.actor_target, self.actor_target)
         utils.hard_update_params(other.critic, self.critic)
         utils.hard_update_params(other.critic_target, self.critic_target)
         
+        if self.init_critic:
+            utils.soft_update_params(other.value.trunk, self.value.trunk)
     
     def get_meta_specs(self):
         return tuple()
diff --git a/agent/smm.py b/agent/smm.py
index 41f1a2c..bc38783 100644
--- a/agent/smm.py
+++ b/agent/smm.py
@@ -174,6 +174,12 @@ class SMMAgent(DDPGAgent):
         meta = OrderedDict()
         meta['z'] = z
         return meta
+    
+    def init_all_meta(self):
+        z = np.eye(self.z_dim, dtype=np.float32)
+        meta = OrderedDict()
+        meta['z'] = z
+        return meta
 
     def update_meta(self, meta, global_step, time_step):
         # during fine-tuning, find the best skill and fine-tune that one only.
@@ -251,7 +257,7 @@ class SMMAgent(DDPGAgent):
         obs_z = torch.cat([obs, z], dim=1)  # do not learn encoder in the VAE
         next_obs_z = torch.cat([next_obs, z], dim=1)
 
-        if self.reward_free:
+        if True: # always reward free for pretraining for EDL; self.reward_free:
             vae_metrics, h_s_z = self.update_vae(obs_z)
             pred_metrics, h_z_s = self.update_pred(obs.detach(), z)
 
diff --git a/envs/make_maze.py b/envs/make_maze.py
index 18b0248..0e47db6 100755
--- a/envs/make_maze.py
+++ b/envs/make_maze.py
@@ -61,8 +61,16 @@ class DMCStyleWrapper:
     def action_range(self):
         return self.act_range
 
-    def reset(self, state=None):
-        self._env.reset()
+    def reset(self, state=None, goal=None):
+        if self.maze_type in ['AntU','AntFb','AntMaze']:
+            self._env.reset() # observation, desired_goal
+            if state is not None:
+                self._env._cur_obs['observation'] = state
+            if goal is not None:
+                self._env._full_state_goal = np.array(goal.squeeze(0))
+        else:
+            self._env.reset(state=state, goal=goal) # observation, desired_goal
+
 
         if self.maze_type in ['AntU','AntFb','AntMaze']:
             obs = self._env._cur_obs['observation'].astype(self.obs_dtype)
@@ -71,12 +79,12 @@ class DMCStyleWrapper:
             else:
                 prev_obs = self._env._prev_obs['observation'].astype(self.obs_dtype)
 
-            return MakeTimestep(self.maze_type, obs, self.maximum_timestep, 
+            return MakeTimestep(self.maze_type, obs, self.maximum_timestep,
             self._env.timesteps_so_far, prev_obs)
 
         else:
             return MakeTimestep(self.maze_type, self._env._state, self.maximum_timestep)
-    
+ 
     def step(self, action):
         self._env.step(action)
 
diff --git a/envs/mazes.py b/envs/mazes.py
index 34e2e04..b58e6f6 100755
--- a/envs/mazes.py
+++ b/envs/mazes.py
@@ -8,7 +8,6 @@ import numpy as np
 import matplotlib.pyplot as plt
 import ipdb
 from sklearn.neighbors import KernelDensity
-from scipy.interpolate import interp1d
 
 plot_colors = ['blue', 'red', 'green', 'brown', 'orange', 'pink', 'rebeccapurple', 'olive', 'purple', 'cyan', 
                'black', 'maroon', 'yellow', 'navy', 'darkgreen', 'lavender', 'peru', 'fuchsia', 'slateblue', 'oldlace',
@@ -301,7 +300,8 @@ class Maze:
                         save_dir = None, 
                         step = 0, 
                         use_wandb=False,
-                        env=None):
+                        env=None,
+                        goal=None):
         plt.clf()
         if ax is None:
             f, ax = plt.subplots(1, 1, figsize=(6, 4))
@@ -313,18 +313,44 @@ class Maze:
             color = plot_colors[idx % 50]
             for movement in trajectory:
                 ax.plot(movement[0], movement[1], color = color, linewidth = 0.3)
+            ax.scatter(goal[idx][:,0],goal[idx][:,1], c = color, s = 100)
 
         ax.legend(loc = 'center left', fontsize = 'xx-small', ncol=2, bbox_to_anchor=(1, 0.5))
         plt.savefig(save_dir)
         if use_wandb:
             wandb_img = wandb.Image(save_dir.__str__())
             wandb.log({'eval_trajectory': wandb_img})
+            # wandb.log({'eval_trajectory_%d'%(step): wandb_img})
         plt.clf()
         plt.cla()
         f.clear()
         plt.close(f)
+        
+        plt.clf()
+        f2, ax2 = plt.subplots(1, 1, figsize=(6, 4))
+        plt.subplots_adjust(bottom=0.1, right=0.7, top=0.9)
+        for x, y in self._walls:
+            ax2.plot(x, y, 'k-')
+
+        for idx, trajectory in trajectory_all.items():
+            color = plot_colors[idx % 50]
+            for movement in trajectory:
+                ax2.plot(movement[0], movement[1], color = color, linewidth = 0.3)
+
+        ax2.legend(loc = 'center left', fontsize = 'xx-small', ncol=2, bbox_to_anchor=(1, 0.5))
+        save_dir2 = save_dir.__str__().replace('.png', '_without_goal.png')
+        plt.savefig(save_dir2)
+        if use_wandb:
+            wandb_img2 = wandb.Image(save_dir2.__str__())
+            wandb.log({'eval_trajectory_without_goal': wandb_img2})
+            # wandb.log({'eval_trajectory_%d'%(step): wandb_img})
+        plt.clf()
+        plt.cla()
+        f.clear()
+        plt.close(f2)
 
-    def state_coverage_1(self, trajectory_all, skill_dim):          
+    def state_coverage(self, trajectory_all, skill_dim):        
+        state_coveraged = 0
         state_cov = set()
         for n in range(skill_dim):
             for i in np.arange(-0.5,9.5):
@@ -332,27 +358,9 @@ class Maze:
                     ob = np.array(trajectory_all[n])[:,:,0]
                     if ob[((ob[:,0]<i+1) & (ob[:,0]>=i) & (ob[:,1]<j+1) & (ob[:,1]>=j))].sum() >= 1:
                         state_cov.add((i,j))
-        return len(state_cov)
-
-    def state_coverage_2(self, trajectory_all, skill_dim):          
-        state_cov = set()
-        all_dots = []
-        for idx, trajectory in trajectory_all.items():
-            for movement in trajectory:
-                mv = np.array(movement)
-                x = mv[0]
-                y = mv[1]
-                f = interp1d(x,y)
-                x_new = np.linspace(x[0],x[1],30)
-                dots = np.concatenate((x_new.reshape(-1,1), f(x_new).reshape(-1,1)), axis=1)
-                all_dots.append(dots)
-
-        for ob in all_dots:
-            for i in np.arange(-0.5,9.5):
-                for j in np.arange(-0.5, 9.5):
-                    if ob[((ob[:,0]<i+1) & (ob[:,0]>=i) & (ob[:,1]<j+1) & (ob[:,1]>=j))].sum() >= 1:
-                        state_cov.add((i,j))
-        return len(state_cov)
+            state_coveraged += len(state_cov)
+        state_coveraged_avg = state_coveraged / skill_dim
+        return state_coveraged_avg
 
     def sample(self):
         segment_keys = list(self._segments.keys())
@@ -424,6 +432,7 @@ class Maze:
                 break
         return loc[0], loc[1]
 
+
     def sample_goal(self, min_wall_dist=None):
         if min_wall_dist is None:
             min_wall_dist = 0.1
@@ -441,45 +450,6 @@ class Maze:
                 break
         return loc[0], loc[1]
 
-
-    def eval_sample_goal(self, min_wall_dist):
-        # if min_wall_dist is None:
-        #     min_wall_dist = 0.1
-        # else:
-        #     min_wall_dist = min(0.4, max(0.01, min_wall_dist))
-
-        goal_candidates = [i for i in self._segments]
-        goal_idx = np.random.randint(low=0, high=len(goal_candidates))
-        if goal_idx == 0:
-            g_square_loc = (0.0, 0.0)
-        else:
-            g_square_loc = tuple(map(float, goal_candidates[goal_idx].split(',')))
-
-        while True:
-            shift = np.random.uniform(low=-0.5, high=0.5, size=(2,))
-            goal_candidate = g_square_loc + shift
-
-            dist_1 = np.array([min_wall_dist, 0])
-            dist_2 = np.array([0, min_wall_dist])
-            dist_3 = np.array([-min_wall_dist, 0])
-            dist_4 = np.array([0, -min_wall_dist])
-            stop_1 = self.move(goal_candidate, dist_1)
-            stop_2 = self.move(goal_candidate, dist_2)
-            stop_3 = self.move(goal_candidate, dist_3)
-            stop_4 = self.move(goal_candidate, dist_4)
-
-            is_same = 0
-            is_same += float(np.sum(np.abs((goal_candidate + dist_1) - stop_1)))
-            is_same += float(np.sum(np.abs((goal_candidate + dist_2) - stop_2)))
-            is_same += float(np.sum(np.abs((goal_candidate + dist_3) - stop_3)))
-            is_same += float(np.sum(np.abs((goal_candidate + dist_4) - stop_4)))
-
-            if is_same == 0: 
-                break
-        return goal_candidate[0], goal_candidate[1]
-
-
-
     def move(self, coord_start, coord_delta, depth=None):
         if depth is None:
             depth = 0
@@ -905,7 +875,7 @@ mazes_dict['square_bottleneck'] = {'maze': Maze(*segments_crazy, goal_squares='9
 
 mazes_dict['square_upside'] = {'maze': Maze(*segments_crazy, goal_squares='9,9', min_wall_coord=4,
                                                 walls_to_remove=_walls_to_remove, walls_to_add=_walls_to_add),
-                                   'action_range': 0.2}
+                                   'action_range': 0.19}
 
 segments_empty = [
     dict(name='A', anchor='origin', direction='right', times=4),
diff --git a/multiworld/envs/mujoco/classic_mujoco/ant.py b/multiworld/envs/mujoco/classic_mujoco/ant.py
index 04702b7..9c02e0e 100755
--- a/multiworld/envs/mujoco/classic_mujoco/ant.py
+++ b/multiworld/envs/mujoco/classic_mujoco/ant.py
@@ -208,15 +208,10 @@ class AntEnv(MujocoEnv, Serializable, MultitaskEnv, metaclass=abc.ABCMeta):
 
         # self.reset()
 
-    def step(self, action):
-        self._prev_obs = self._cur_obs
-        flipped_before_step = self.is_flipped
-        self.do_simulation(np.array(action), self.frame_skip)
-        self.timesteps_so_far += 1
-        ob = self._get_obs()
+    def make_reward_info(self, action, ob):
         reward = self.compute_reward(action, ob)
-        info = {}
         # if self._full_state_goal is not None:
+        info = {}
         info['full-state-distance'] = self._compute_state_distances(
             self.numpy_batchify_dict(ob)
         )
@@ -254,6 +249,18 @@ class AntEnv(MujocoEnv, Serializable, MultitaskEnv, metaclass=abc.ABCMeta):
         info['leg-distance'] = self._compute_leg_distances(
             self.numpy_batchify_dict(ob)
         )
+        return reward, info
+
+    def step(self, action):
+        self._prev_obs = self._cur_obs
+        flipped_before_step = self.is_flipped
+        self.do_simulation(np.array(action), self.frame_skip)
+        self.timesteps_so_far += 1
+        ob = self._get_obs()
+        try:
+            reward, info = self.compute_reward(action, ob)
+        except:
+            reward, info = 0.0, {}
         if self.terminate_when_unhealthy:
             done = not self.is_healthy
             reward += self._healthy_reward
diff --git a/multiworld/envs/mujoco/classic_mujoco/ant_maze.py b/multiworld/envs/mujoco/classic_mujoco/ant_maze.py
index 481332b..8ccc4ab 100755
--- a/multiworld/envs/mujoco/classic_mujoco/ant_maze.py
+++ b/multiworld/envs/mujoco/classic_mujoco/ant_maze.py
@@ -33,7 +33,7 @@ class AntMazeEnv(AntEnv):
 
         if model_path in [
             'classic_mujoco/ant_maze2_gear30_small_dt3.xml',
-            'classic_mujoco/ant_gear30_dt3_u_small.xml',
+            'classic_mujoco/ant_gear30_dt3_u_small.xml',  
         ]:
             self.maze_type = 'u-small'
         elif model_path in [
@@ -613,9 +613,17 @@ class AntMazeEnv(AntEnv):
 
         if self.model_path in [
             'classic_mujoco/ant_maze2_gear30_small_dt3.xml',
-            'classic_mujoco/ant_gear30_dt3_u_small.xml',
+            'classic_mujoco/ant_gear30_dt3_u_small.xml', # BK: u-small
         ]:
             extent = [-3.5, 3.5, -3.5, 3.5]
+        elif self.model_path in [  #BK: fb-small
+            'classic_mujoco/ant_fb_gear30_small_dt3.xml',
+        ]:
+            extent = [-5.25, 5.25, -5.25, 5.25]
+        elif self.model_path in [  # BK: maze-small
+            'classic_mujoco/ant_gear10_dt3_maze_small.xml'
+        ]:
+            extent = [-4, 4, -4, 4]
         elif self.model_path in [
             'classic_mujoco/ant_gear30_dt3_u_med.xml',
             'classic_mujoco/ant_gear15_dt3_u_med.xml',
@@ -802,10 +810,6 @@ class Wall:
         return (self.min_x < points[:,0]) * (points[:,0] < self.max_x) \
                * (self.min_y < points[:,1]) * (points[:,1] < self.max_y)
 
-    # # BK
-    # def contains_point_ths(self, point, ths):
-    #     ipdb.set_trace()
-    #     return ((self.min_x < point[0]+ths) and (point[0]-ths<self.max_x)) and ((self.min_y < point[1]+ths) and (point[1]-ths<self.max_y))
 
 
 if __name__ == '__main__':
diff --git a/pretrain_maze.py b/pretrain_maze.py
index 00c5781..60ccf30 100644
--- a/pretrain_maze.py
+++ b/pretrain_maze.py
@@ -16,6 +16,7 @@ import torch
 import wandb
 import imageio
 from collections import OrderedDict
+from tqdm import tqdm
 
 import utils
 import omegaconf
@@ -47,6 +48,7 @@ class Workspace:
         self.cfg = cfg
         self.maze_type = cfg.maze_type
         self.dtype = cfg.dtype
+        self.sibling_epsilon = cfg.sibling_epsilon
         
         utils.set_seed_everywhere(cfg.seed)
         self.device = torch.device(cfg.device)
@@ -62,12 +64,23 @@ class Workspace:
                              use_wandb=cfg.use_wandb)
         # create envs
         if cfg.maze_type in ['AntU','AntFb','AntMaze']:
+            self.train_env0, _ = \
+                        make_maze.make_antmaze(cfg.maze_type, cfg.maximum_timestep, 
+                                                cfg.dtype, is_pretrain=True)
             self.train_env, self.eval_env = \
                         make_maze.make_antmaze(cfg.maze_type, cfg.maximum_timestep, 
                                                 cfg.dtype, is_pretrain=True)
+            if cfg.sibling_rivalry:
+                self.train_env2, _ = \
+                        make_maze.make_antmaze(cfg.maze_type, cfg.maximum_timestep, 
+                                                cfg.dtype, is_pretrain=True)
+                
         else:
+            self.train_env0 = make_maze.make(cfg.maze_type, cfg.maximum_timestep)
             self.train_env = make_maze.make(cfg.maze_type, cfg.maximum_timestep)
             self.eval_env = make_maze.make(cfg.maze_type, cfg.maximum_timestep)
+            if cfg.sibling_rivalry:
+                self.train_env2 = make_maze.make(cfg.maze_type, cfg.maximum_timestep)
 
         # create agent
         self.agent = make_agent(cfg.obs_type,
@@ -88,6 +101,7 @@ class Workspace:
             wandb.init(project="urlb", group=cfg.agent.name, name=name, config=config)
 
         # get meta specs
+        meta_specs_smm = self.agent.smm.get_meta_specs()
         meta_specs = self.agent.get_meta_specs()
         # create replay buffer
         data_specs = (self.train_env.observation_spec(),
@@ -96,15 +110,23 @@ class Workspace:
                       specs.Array((1,), np.float32, 'discount'))
 
         # create data storage
+        self.replay_storage_smm = ReplayBufferStorage(data_specs, meta_specs_smm,
+                                                      self.work_dir / 'buffer')
         self.replay_storage = ReplayBufferStorage(data_specs, meta_specs,
                                                   self.work_dir / 'buffer')
 
         # create replay buffer
+        self.replay_loader_smm = make_replay_loader(self.replay_storage_smm,
+                                                    cfg.replay_buffer_size,
+                                                    cfg.batch_size,
+                                                    cfg.replay_buffer_num_workers,
+                                                    False, cfg.nstep, cfg.discount)
         self.replay_loader = make_replay_loader(self.replay_storage,
                                                 cfg.replay_buffer_size,
                                                 cfg.batch_size,
                                                 cfg.replay_buffer_num_workers,
                                                 False, cfg.nstep, cfg.discount)
+        self._replay_iter_smm = None
         self._replay_iter = None
 
         # create video recorders
@@ -131,11 +153,112 @@ class Workspace:
         return self.global_step * self.cfg.action_repeat
 
     @property
+    def replay_iter_smm(self):
+        if self._replay_iter_smm is None:
+            self._replay_iter_smm = iter(self.replay_loader_smm)
+        return self._replay_iter_smm
+
+    @property
     def replay_iter(self):
         if self._replay_iter is None:
             self._replay_iter = iter(self.replay_loader)
         return self._replay_iter
 
+    def pretrain_eval(self):
+        step, episode, total_reward = 0, 0, 0
+        meta_all = self.agent.smm.init_all_meta()
+        meta = OrderedDict()
+        trajectory_all = {}
+        total_diayn_rw = 0
+
+        # 한 skill당 몇번 rollout 할건지
+        if self.maze_type in ['AntU','AntFb','AntMaze']:
+            num_eval_each_skill = 1
+        else:
+            num_eval_each_skill = 5
+
+        # 총 몇개의 skill을 rollout 할건지
+        if self.cfg.agent.name in ['dads', 'aps']:
+            num_eval_skills = self.agent.eval_num_skills
+        else:
+            num_eval_skills = self.agent.smm.z_dim
+
+
+        for episode in range(self.agent.smm.z_dim):
+            meta['z'] = meta_all['z'][episode]
+            trajectory = []
+            time_step = self.eval_env.reset()
+            if (self.maze_type in ['AntU','AntFb','AntMaze']) & (episode<10): #VIDEO
+                self.video_recorder.init_ant(self.eval_env, enabled=True)
+
+            for idx in range(num_eval_each_skill):
+                time_step = self.eval_env.reset()
+                while not time_step.last():
+                    with torch.no_grad(), utils.eval_mode(self.agent.smm):
+                        action = self.agent.smm.act(time_step.observation,
+                                                    meta,
+                                                    self.global_step,
+                                                    eval_mode=True)
+
+                    time_step = self.eval_env.step(action)
+                    if (episode<10) & (self.maze_type in ['AntU','AntFb','AntMaze']): #VIDEO
+                        if idx==0:
+                            self.video_recorder.record_ant(self.eval_env)
+                    trajectory.append([[time_step.prev_observation[0].item(), time_step.observation[0].item()],
+                                    [time_step.prev_observation[1].item(), time_step.observation[1].item()]])
+                    total_reward += time_step.reward
+
+                # terminal state에서만 diayn_rw를 재서 learned_skill 개수를 구한다
+                if self.cfg.agent.name in ['diayn', 'disdain']:
+                    with torch.no_grad():
+                        diayn_rw = self.agent.compute_intr_reward(
+                            torch.tensor(meta['skill']).unsqueeze(0).to(self.device),
+                            torch.tensor(time_step.observation).unsqueeze(0).to(self.device)
+                        )
+                    diayn_rw = diayn_rw.item()
+                    total_diayn_rw += diayn_rw
+                else: #EDL, DADS, APS는 못재
+                    pass
+
+            trajectory_all[episode] = trajectory
+            if (self.maze_type in ['AntU','AntFb','AntMaze']) & (episode<10): #VIDEO
+                self.video_recorder.save(f'skill_{episode}_frame_{self.global_frame}.mp4')
+
+
+        save_dir = self.get_dir(f'{self.exp_name}/{self.exp_name}_{self.global_frame}.png')
+
+        self.eval_env.plot_trajectory(trajectory = trajectory_all,
+                                    save_dir = save_dir,
+                                    step = self.global_step,
+                                    use_wandb = self.cfg.use_wandb)
+
+        # Ant 사진뽑기
+        # dummy_img = self.train_env._env.get_image(width=168,height=168)
+        # rgb_img = self.train_env._env.get_image(width=168,height=168)
+        # plot_img = self.train_env._env.get_image_plt(imsize=400, draw_walls=True, draw_state=True, draw_goal=False, draw_subgoals=False)
+        # imageio.imwrite('abcd.png', rgb_img)
+
+        # check state coverage (10x10 격자를 몇개 채웠는지)
+        state_coveraged_1 = self.eval_env.state_coverage_1(trajectory_all=trajectory_all,
+                                                           skill_dim=self.agent.smm.z_dim)
+        state_coveraged_2 = self.eval_env.state_coverage_2(trajectory_all=trajectory_all,
+                                                           skill_dim=self.agent.smm.z_dim)
+        num_learned_skills = np.exp(total_diayn_rw / (self.agent.skill_dim * num_eval_each_skill))
+
+        if self.maze_type in ['AntU','AntFb','AntMaze']:
+            num_bucket = 150
+        else:
+            num_bucket = 100
+        with self.logger.log_and_dump_ctx(self.global_frame, ty='eval') as log:
+            log('episode_reward', total_reward / (episode*num_eval_each_skill))
+            # log('episode_length', step * self.cfg.action_repeat / episode)
+            log('episode', self.global_episode)
+            log('step', self.global_step)
+            log(f'state_coveraged(out of {num_bucket} bucekts)', state_coveraged_1)
+            log(f'new_state_coveraged(out of {num_bucket} bucekts)', state_coveraged_2)
+            log('num_learned_skills', num_learned_skills)
+
+
     def eval(self):
         step, episode, total_reward = 0, 0, 0
         meta_all = self.agent.init_all_meta()
@@ -150,21 +273,22 @@ class Workspace:
             num_eval_each_skill = 5 
 
         # 총 몇개의 skill을 rollout 할건지
-        # if self.cfg.agent.name in ['dads', 'aps']:
-        num_eval_skills = self.agent.eval_num_skills
-        # else:
-        #     num_eval_skills = self.agent.skill_dim
+        if self.cfg.agent.name in ['dads', 'aps']:
+            num_eval_skills = self.agent.eval_num_skills
+        else:
+            num_eval_skills = self.agent.skill_dim
 
 
         for episode in range(num_eval_skills):
             meta['skill'] = meta_all['skill'][episode]
             trajectory = []
-            time_step = self.eval_env.reset()
+            goal = self.agent.vae.get_centroids(torch.tensor(episode).to(self.device)).detach().cpu()
+            time_step = self.eval_env.reset(goal=goal)
             if (self.maze_type in ['AntU','AntFb','AntMaze']) & (episode<10): #VIDEO
                 self.video_recorder.init_ant(self.eval_env, enabled=True)
 
             for idx in range(num_eval_each_skill): 
-                time_step = self.eval_env.reset()
+                time_step = self.eval_env.reset(goal=goal)
                 while not time_step.last():
                     with torch.no_grad(), utils.eval_mode(self.agent):
                         action = self.agent.act(time_step.observation,
@@ -230,6 +354,101 @@ class Workspace:
             log(f'new_state_coveraged(out of {num_bucket} bucekts)', state_coveraged_2)
             log('num_learned_skills', num_learned_skills)
 
+    def distance_reward(self, transition, goal, antigoal):
+        if self.cfg.maze_type == 'AntU':
+            drg = -self.dist(torch.tensor(transition.observation[:2]), goal)
+            dra = -self.dist(torch.tensor(transition.observation[:2]), antigoal)
+        else:
+            drg = -self.train_env._env.dist(torch.tensor(transition.observation), goal)
+            dra = -self.train_env._env.dist(torch.tensor(transition.observation), antigoal)
+        return torch.clamp(drg - dra, -np.inf, 0)
+
+    def sample_dataset(self, env, condition_fn=lambda x: True):
+        dataset = np.zeros((self.cfg.oracle_num_samples, 2))
+        for sample_idx in range(self.cfg.oracle_num_samples):
+            done = False
+            while not done:
+                s = env.sample()
+                done = condition_fn(s)
+            dataset[sample_idx] = np.array(s)
+        dataset = torch.from_numpy(dataset).float().to(self.cfg.device)
+        return dataset
+
+    def pretrain(self):
+        train_until_step = utils.Until(self.cfg.num_pretrain_frames,
+                                       self.cfg.action_repeat)
+        seed_until_step = utils.Until(self.cfg.num_seed_frames,
+                                      self.cfg.action_repeat)
+        eval_every_step = utils.Every(self.cfg.eval_every_frames,
+                                      self.cfg.action_repeat)
+
+        if self.maze_type in ['AntU','AntFb','AntMaze']:
+            self.train_env0._env.get_image(width=200, height=200)
+
+        episode_step, episode_reward = 0, 0
+        time_step = self.train_env0.reset()
+        time_step.action = np.zeros(self.agent.smm.action_dim, dtype=time_step.observation.dtype)
+        meta = self.agent.smm.init_meta()
+        self.replay_storage_smm.add(time_step, meta)
+        metrics = None
+        while train_until_step(self.global_step):
+            if time_step.last():
+                self._global_episode += 1
+                # wait until all the metrics schema is populated
+                if metrics is not None:
+                    # log stats
+                    elapsed_time, total_time = self.timer.reset()
+                    episode_frame = episode_step * self.cfg.action_repeat
+                    with self.logger.log_and_dump_ctx(self.global_frame,
+                                                      ty='train') as log:
+                        log('fps', episode_frame / elapsed_time)
+                        log('total_time', total_time)
+                        log('episode_reward', episode_reward)
+                        log('episode_length', episode_frame)
+                        log('episode', self.global_episode)
+                        log('buffer_size', len(self.replay_storage_smm))
+                        log('step', self.global_step)
+
+                # reset env
+                time_step = self.train_env0.reset()
+                time_step.action = np.zeros(self.agent.smm.action_dim, dtype=time_step.observation.dtype)
+                meta = self.agent.smm.init_meta()
+                self.replay_storage_smm.add(time_step, meta)
+                # try to save snapshot
+                if self.global_frame in self.cfg.snapshots:
+                    self.save_snapshot()
+                episode_step = 0
+                episode_reward = 0
+
+            # try to evaluate
+            if eval_every_step(self.global_step):
+                self.logger.log('eval_total_time', self.timer.total_time(),
+                                self.global_frame)
+                self.pretrain_eval()
+
+            # sample action
+            with torch.no_grad(), utils.eval_mode(self.agent.smm):
+                action = self.agent.smm.act(time_step.observation,
+                                        meta,
+                                        self.global_step,
+                                        eval_mode=False)
+
+            # try to update the agent
+            if not seed_until_step(self.global_step):
+                metrics = self.agent.smm.update(self.replay_iter_smm, self.global_step)
+                self.logger.log_metrics(metrics, self.global_frame, ty='train')
+
+            # take env step
+            time_step = self.train_env0.step(action)
+            time_step.action = action
+            episode_reward += time_step.reward
+            self.replay_storage_smm.add(time_step, meta)
+            episode_step += 1
+            self._global_step += 1
+
+    def dist(self, goal, outcome):
+        return torch.sqrt(torch.sum(torch.pow(goal - outcome, 2)))
+
     def train(self):
         # predicates
         train_until_step = utils.Until(self.cfg.num_train_frames,
@@ -241,16 +460,97 @@ class Workspace:
 
         # discard dummy_img. 처음 get_image 했을 때 zero-array가 나옴
         if self.maze_type in ['AntU','AntFb','AntMaze']:
-            self.train_env._env.get_image(width=200, height=200)
-        
+            self.train_env._env.get_image(width=200, height=200) 
+
+        # SMM train
+        self.pretrain()
+        self._global_step = 0
+        dataset = list()
+        for _ in range(len(self.replay_storage_smm)//self.cfg.batch_size):
+            batch = next(self.replay_iter_smm)
+            _, _, _, _, obs, _ = batch
+            dataset.append(torch.tensor(obs[:, :2]).to(self.cfg.device))
+        dataset = torch.stack(dataset).reshape(-1, 2) # self.cfg['agent']['obs_shape'][0])
+
+        # oracle train
+        # dataset = self.sample_dataset(self.train_env._env)
+        # self.agent.vae.update_normalizer(dataset=dataset)
+        indices = list(range(dataset.size(0)))
+        for iter_idx in tqdm(range(self.cfg.oracle_dur), desc="Training"):
+            # Make batch
+            batch_indices = np.random.choice(indices, size=self.cfg.batch_size)
+            # only xy for goal
+            batch = dict(next_state=dataset[batch_indices])
+            metrics = self.agent.update_vae(batch)
+            self.logger.log_metrics(metrics, self.global_frame, ty='train')
+ 
         episode_step, episode_reward = 0, 0
-        time_step = self.train_env.reset()
-        time_step.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
         meta = self.agent.init_meta()
+        goal = self.agent.vae.get_centroids(torch.tensor(meta['skill'].argmax()).to(self.cfg.device)).detach().cpu()
+        time_step = self.train_env.reset(goal=goal)
+        time_step.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
+        if self.cfg.sibling_rivalry:
+            time_step2 = time_step
+            self.train_env2.reset(state=time_step.observation, goal=goal)
+            self.train_env._env.set_state(self.train_env._env.sim.data.qpos, self.train_env._env.sim.data.qvel)
+            time_step2.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
         self.replay_storage.add(time_step, meta)
         metrics = None
+        _compress_me = list()
+        is_success, is_success2 = False, False
         while train_until_step(self.global_step):
             if time_step.last():
+                if self.cfg.sibling_rivalry:
+                    achieved0 = torch.stack([x[2] for x in _compress_me])
+                    achieved1 = torch.stack([x[3] for x in _compress_me])
+                    success0 = torch.stack([x[4] for x in _compress_me])
+                    success1 = torch.stack([x[5] for x in _compress_me])
+                    success0 = True in success0
+                    success1 = True in success1
+                    # antigoal0 = achieved1
+                    # antigoal1 = achieved0
+
+                    # cur_goal = self.train_env._env.goal.detach()
+                    # goal = cur_goal.unsqueeze(0).repeat(achieved0.shape[0], 1)
+                    cur_goal = goal
+                    goal = goal.repeat(achieved0.shape[0], 1)
+
+                    if self.maze_type in ['AntU','AntFb','AntMaze']:
+                        is_0_closer = self.dist(goal, achieved0) < self.dist(goal, achieved1)
+                        within_epsilon = self.dist(achieved0, achieved1) < self.sibling_epsilon
+                    else:
+                        is_0_closer = self.train_env._env.dist(goal, achieved0) < self.train_env._env.dist(goal, achieved1)
+                        within_epsilon = self.train_env._env.dist(achieved0, achieved1) < self.sibling_epsilon
+
+                    if is_0_closer:
+                        include0 = bool(within_epsilon) or success0
+                        include1 = True
+                    else:
+                        include0 = True
+                        include1 = bool(within_epsilon) or success1
+
+                    ep_tuples = [
+                        (0, [(x[0], x[3], x[6]) for x in _compress_me], include0, cur_goal),
+                        (1, [(x[1], x[2], x[6]) for x in _compress_me], include1, cur_goal)
+                    ]
+
+                    for ai, ep, include, g in ep_tuples:
+                        if include:
+                            for time_step, ag, meta in ep:
+                                if time_step.last():
+                                    time_step.reward *= 0
+                                    if (ai == 0 and success0) or (ai == 1 and success1):
+                                        time_step.reward += 1
+                                    else:
+                                        time_step.reward += self.distance_reward(time_step, g, ag).item()
+                                else:
+                                    time_step.reward *= 0
+                                    if (ai == 0 and success0) or (ai == 1 and success1):
+                                        time_step.reward += 1
+
+                                self.replay_storage.add(time_step, meta)
+                    _compress_me = list()
+
                 self._global_episode += 1
                 # wait until all the metrics schema is populated
                 if metrics is not None:
@@ -268,16 +568,38 @@ class Workspace:
                         log('step', self.global_step)
 
                 # reset env
-                time_step = self.train_env.reset()
-                time_step.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
                 meta = self.agent.init_meta()
-                self.replay_storage.add(time_step, meta)
+                goal = self.agent.vae.get_centroids(torch.tensor(meta['skill'].argmax()).to(self.cfg.device)).detach().cpu()
+                time_step = self.train_env.reset(goal=goal)
+                time_step.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
+                if self.cfg.sibling_rivalry:
+                    time_step2 = self.train_env2.reset(state=time_step.observation, goal=goal)
+                    time_step2.action = np.zeros(self.agent.action_dim, dtype=time_step.observation.dtype)
+                    if self.cfg.maze_type == 'AntU':
+                        diff = time_step.observation[:2] - np.array(goal[0, :2])
+                        is_success = (np.linalg.norm(diff, ord=2) < 1.50).astype(int) if not is_success else is_success
+                        diff2 = time_step2.observation[:2] - np.array(goal[0, :2])
+                        is_success2 = (np.linalg.norm(diff2, ord=2) < 1.50).astype(int) if not is_success2 else is_success2
+                        achieved = goal[0] if is_success else torch.tensor(time_step.observation[:2])
+                        achieved2 = goal[0] if is_success2 else torch.tensor(time_step2.observation[:2])
+                        _compress_me.append([time_step, time_step2,
+                                             achieved, achieved2,
+                                             torch.tensor(is_success), torch.tensor(is_success2),
+                                             meta])
+                    else:
+                        _compress_me.append([time_step, time_step2,
+                                             self.train_env._env.achieved, self.train_env2._env.achieved,
+                                             self.train_env._env.is_success, self.train_env2._env.is_success,
+                                             meta])
+                else:
+                    self.replay_storage.add(time_step, meta)
                 # try to save snapshot
                 if self.global_frame in self.cfg.snapshots:
                     print('snapshot 저장', self.global_frame)
                     self.save_snapshot()
                 episode_step = 0
                 episode_reward = 0
+                is_success, is_success2 = False, False
 
             # try to evaluate
             if eval_every_step(self.global_step):
@@ -301,7 +623,27 @@ class Workspace:
             time_step = self.train_env.step(action)
             time_step.action = action
             episode_reward += time_step.reward
-            self.replay_storage.add(time_step, meta)
+            if self.cfg.sibling_rivalry:
+                time_step2 = self.train_env2.step(action)
+                time_step2.action = action
+                if self.cfg.maze_type == 'AntU':
+                    diff = time_step.observation[:2] - np.array(goal[0, :2])
+                    is_success = (np.linalg.norm(diff, ord=2) < 1.50).astype(int) if not is_success else is_success
+                    diff2 = time_step2.observation[:2] - np.array(goal[0, :2])
+                    is_success2 = (np.linalg.norm(diff2, ord=2) < 1.50).astype(int) if not is_success2 else is_success2
+                    achieved = goal[0] if is_success else torch.tensor(time_step.observation[:2])
+                    achieved2 = goal[0] if is_success2 else torch.tensor(time_step2.observation[:2])
+                    _compress_me.append([time_step, time_step2,
+                                         achieved, achieved2,
+                                         torch.tensor(is_success), torch.tensor(is_success2),
+                                         meta])
+                else:
+                    _compress_me.append([time_step, time_step2,
+                                         self.train_env._env.achieved, self.train_env2._env.achieved,
+                                         self.train_env._env.is_success, self.train_env2._env.is_success,
+                                         meta])
+            else:
+                self.replay_storage.add(time_step, meta)
             episode_step += 1
             self._global_step += 1
 
diff --git a/pretrain_maze.yaml b/pretrain_maze.yaml
index c50f823..4ac5461 100644
--- a/pretrain_maze.yaml
+++ b/pretrain_maze.yaml
@@ -1,5 +1,5 @@
 defaults:
-  - agent: diayn
+  - agent: edl
   - override hydra/launcher: submitit_local
 
 # mode
@@ -11,13 +11,14 @@ frame_stack: 3 # only works if obs_type=pixels
 action_repeat: 1 # set to 2 for pixels
 discount: 0.99
 # train settings
-num_train_frames: 2000010
+num_pretrain_frames: 500010
+num_train_frames: 2500010
 num_seed_frames: 4000
 # eval
 eval_every_frames: 10000
 num_eval_episodes: 10
 # snapshot
-snapshots: []
+snapshots: [100000, 500000, 1000000, 2000000, 2500000, 4000000, 6000000, 8000000, 9000000]
 snapshot_dir: ../../../pretrained_models/${maze_type}/${agent.name}/${seed}
 # replay buffer
 replay_buffer_size: 1000000
@@ -41,6 +42,12 @@ experiment: exp
 maximum_timestep: 50
 maze_type: square_bottleneck
 
+sibling_rivalry: true
+sibling_epsilon: 9.5
+
+oracle_num_samples: 4096
+oracle_dur : 50000
+
 hydra:
   run:
     dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${agent.name}_${maze_type}_skd_${agent.skill_dim}
